{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1od8qeOhdyy2JGTKDLgN3P5XIi5ox9iZi",
      "authorship_tag": "ABX9TyPuYAAW/gpwfHBeDYUmvguc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JhonathanOrtiz/NLP/blob/master/Seq2Seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi5cISILUuGM",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> # <strong>  Introducción </strong>\n",
        "\n",
        "<h1>Spanish</h1>\n",
        "\n",
        "La siguiente aplicación tiene como finalidad mediante un modelo Seq2Seq mapear desde un conjunto de entrada los cuales serán oraciones de problemas matemáticos a su respectiva equación.\n",
        "\n",
        "Ya que el texto es un tipo de dato categorizado como serie de tiempo porque las letras y palabras tienen una realción entre si utilizamos modelos de Redes Neuronales Recurrentes porque ellas son capaces de recordar información. [Aqui puedes profundizar](https://towardsdatascience.com/understanding-rnn-and-lstm-f7cdf6dfc14e)\n",
        "\n",
        "Cuando se trabaja con texto tenemos varios tipos de modelos, aquellos donde desde una secuencia queremos predecir alguna característica en particular *Many2One*, podemos querer desde una característica representar una secuencia *One2Many* o desde una Sequencia predecir otra Secuncia *Many2Many* nosotros nos centraremos en ese modelo ya que a partir de una secuencia de texto queremos (Oración) predecir otra sequencia (Ecuación)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKulaovcXS6O",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> # <strong>  Introduction </strong>\n",
        "\n",
        "<h1>English</h1>\n",
        "\n",
        "The following application is intended to map from a set of inputs, which will be sentences of mathematical problems to their respective equation, using a Seq2Seq model.\n",
        "\n",
        "Since text is a type of data categorized as a time series because letters and words have a relationship with each other we use Recurrent Neural Network models because they are able to remember information. [Here you can go deeper](https://towardsdatascience.com/understanding-rnn-and-lstm-f7cdf6dfc14e)\n",
        "\n",
        "When we work with text we have several types of models, those where from a sequence we want to predict some particular characteristic *Many2One*, we can want from a characteristic to represent a *One2Many* sequence or from a Sequence to predict another *Many2Many* Section we will focus on that model since from a text sequence we want (Sentence) to predict another sequence (Equation)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQMF9CuSj3If",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "outputId": "f44b734a-ed98-4569-ad15-068c722b7030"
      },
      "source": [
        "!python -m spacy download es"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: es_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.2.5/es_core_news_sm-2.2.5.tar.gz#egg=es_core_news_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from es_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (49.1.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/es_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/es\n",
            "You can now load the model via spacy.load('es')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5N5wZM7Mrrd_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d68ba1a8-13ac-441a-8e0e-84e812090967"
      },
      "source": [
        "%cd /content/drive/My Drive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVYZAAq-jocs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from torchtext.data import Field, BucketIterator,TabularDataset\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch import nn\n",
        "import torch\n",
        "from torch import optim\n",
        "import re\n",
        "import numpy as np\n",
        "import random\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pVjkSwmxQXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "esp = spacy.load('es')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tcH5ikwEKe_",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> # Tokenizacion\n",
        "\n",
        "<h1>Spanish</h1>\n",
        "\n",
        "Tokenizar es convertir un texto en una lista de palabras ó caracteres.\n",
        "\n",
        "En éste primer paso definiremos dos funciones, la primera tokenizará por palabras las oraciones ya que son oraciones comunes y corrientes. Sin embargo ya que nuestro target son ecuaciones decidimos tokenizar por caracter.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWZq0WiAFLBT",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> # Tokenization\n",
        "\n",
        "<h1>English</h1>\n",
        "\n",
        "Tokenize transform text into list of word or character.\n",
        "\n",
        "In this first step we define two function, one to tokenize input by word and other to tokenize the target equations by character, since our target are equations and a equantion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ho_l9vFED5_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenizer(text):\n",
        "  return [tok.text.lower() for tok in esp.tokenizer(text) if tok.text != \" \"]\n",
        "  \n",
        "def split_label(text):\n",
        "  label = []\n",
        "  for char in text:\n",
        "    if char != '(' and char != ')' and  char != '[' and char != ']' and char != \"'\" :\n",
        "      label.append(char)\n",
        "  return label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfDAUmQ7GJtL",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> # Dataset\n",
        "\n",
        "<h1>Spanish</h1>\n",
        "\n",
        "Nuestro dataset consta de problemas matematicos como los del colegio, estan en un archivo .csv que yo he preprocesado antes de pasarlo a ese tipo de archivo, para poder usar la librería de [torchtext](https://torchtext.readthedocs.io/en/latest/data.html) puedes chequear la documentación.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZTR1oQUG5U5",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> # Dataset\n",
        "\n",
        "<h1>English</h1>\n",
        "\n",
        "Our dataset is set about math word problem, this datset is a .csv file where one feature (Input) is the math word problem, and the another feature is the equations (Target). Readed With [torchtext](https://torchtext.readthedocs.io/en/latest/data.html) \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltK2SZvmehMx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC = Field(tokenize = tokenizer, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)\n",
        "\n",
        "TRG = Field(tokenize = split_label, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)\n",
        "\n",
        "field = {'Input': ('SRC', SRC), 'Target': ('TRG', TRG)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAEv9745feJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, test_data = TabularDataset.splits(path='/content/drive/My Drive',\n",
        "                                             train = 'dataframe.csv',\n",
        "                                             test = 'dataframe.csv',\n",
        "                                             format='csv',\n",
        "                                             fields= field)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nau35MsMvg2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 1)\n",
        "TRG.build_vocab(train_data, min_freq = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3GxKWW-H_YL",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> # El modelo\n",
        "\n",
        "<h1>Spanish</h1>\n",
        "\n",
        "# <h1>Encoder</h1>\n",
        "\n",
        "\n",
        "Nuestro modelo Seq2Seq lleva como principio una estructura Encoder-Decoder, en esta sección hablaremos del encoder. Cuando pasamos información a través del Encoder el primer paso es una capa *Embedding* ésta será la encargada de mapear la entrada desde una entrada n-dimensional (Esto es el vector one-hot al que corresponde una oración) a un vecor denso con dimensiones que nosotros definiremos.\n",
        "\n",
        "Luego de ésto el siguiente paso es pasar a través del módulo GRU (Gated Recurrect Unit) que es un tipo de LSTM si quieres información sobre ella haz click [aqui](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21) al modulo GRU le pasaremos el vector embedding que creamos en el paso anterior y ésta nos devolverá un output y un hidden state, a diferencia de un modulo LSTM tradicional este no retorna un cell state. Hidden es un vector de tamaño fijo y será el input para el decoder.\n",
        "\n",
        "Hasta ahora... Tenemos un input el cual pasamos por una capa embedding (Para tener representaciones no equidistante de las palaras) ese vector denso será el input de la celda recurrente que mapeará un input a una entrada fija. podríamos decir que estamos aprendiendo una relacion comprimida de la información.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-G88lzcODpS",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        ">  # Build the model\n",
        "\n",
        "<h1>English</h1>\n",
        "<h2>Encoder</h2>\n",
        "\n",
        "Our Seq2Seq model has as a principle an Encoder-Decoder structure, in this section we will talk about the encoder. When we feed information through Encoder.\n",
        "\n",
        " the first step is an Embedding layer which will be in charge of mapping the input from an n-dimensional input (this is the one-hot vector that a sentence corresponds to) to a dense vector with dimensions that we will define.\n",
        "\n",
        "After this the next step is to pass through the GRU (Gated Recurrect Unit) module which is a type of LSTM if you want information about it click [here](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21) to the GRU module we will pass the vector embedding that we created in the previous step and it will return an output and a hidden state, unlike a traditional LSTM module it doesn't return a cell state. Hidden is a fixed size vector and will be the input for the decoder.\n",
        "\n",
        "So far... We have an input which we pass through an embedding layer (To have non equidistant representations of the blades) that dense vector will be the input of the recurrent cell that will map an input to a fixed input. We could say that we are learning a compressed relation of the information.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkPisvzUv0Ab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim) #no dropout as only one layer!\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        \n",
        "        outputs, hidden = self.rnn(embedded) #no cell state!\n",
        "        \n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #outputs are always from the top hidden layer\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq3yXftXOBr2",
        "colab_type": "text"
      },
      "source": [
        "<h1>Decoder</h1>\n",
        "<h2>Spanish</h2>\n",
        "\n",
        "Hemos pasado un vector one-hot que representa una oración a través de un encoder y ese encoder transformó esa representación primero en un vector denso y luego en un vector de tamaño fijo que representa nuestra entrada. Ok ahora ese vector tenemos que transformarlo en otra oración ese es el trabajo de Seq2Seq pasar de una secencia de texto a otra secuencia. En este caso nuestra secuencia son oraciones y por esa razón tokenizamos por character nuestro target asi en cada step el modelo debe predecir un caracter.\n",
        "\n",
        "El Decoder cuenta con una capa de Embedding que hace el mismo trabajo de que el embedding del Encoder. La entrada de esta capa será en el primer paso el token inicial de cada oración y para eso nosotros definimos antes un token para denotar el inicio de la oración < s o s > y en el siguiente step el caracter que hemos predicho y asi iterativamente, por eso debe tener como dimensiones de entrada las dimensiones del vocaculario target.\n",
        "\n",
        "Una capa celda GRU y una capa Fully-Connected que nos dará las dimensiones la probalidad de que un  prediccion pertenezca a una categoría u a otra.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfwJrW41UG63",
        "colab_type": "text"
      },
      "source": [
        "<h1>Decoder</h1>\n",
        "<h2>English</h2>\n",
        "\n",
        "We have feed a one-hot vector representing a sentence through an encoder and that encoder transformed that representation first into a dense vector and then into a fixed size vector representing our input. Ok now that vector we have to transform it into another sentence that is the job of Seq2Seq to pass from one text sequence to another sequence. In this case our sequence is sentences and for that reason we token by character our target so in each step the model must predict a character.\n",
        "\n",
        "The Decoder has an embedding layer that does the same job as the Encoder embedding. The input of this layer will be in the first step the initial token of each sentence and for that we define before a token to denote the beginning of the sentence < s o s > and in the next step the character we have predicted and so iteratively, so it must have as input dimensions the dimensions of the target vocabulary.\n",
        "\n",
        "A GRU cell layer and a Fully-Connected layer that will give us the dimensions the probability that a prediction belongs to one category or another.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYQjZBM3440i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, context):\n",
        "        \n",
        "        #input = [batch size]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #context = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #n layers and n directions in the decoder will both always be 1, therefore:\n",
        "        #hidden = [1, batch size, hid dim]\n",
        "        #context = [1, batch size, hid dim]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "                \n",
        "        emb_con = torch.cat((embedded, context), dim = 2)\n",
        "            \n",
        "        #emb_con = [1, batch size, emb dim + hid dim]\n",
        "            \n",
        "        output, hidden = self.rnn(emb_con, hidden)\n",
        "        \n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #seq len, n layers and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [1, batch size, hid dim]\n",
        "        \n",
        "        output = torch.cat((embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), \n",
        "                           dim = 1)\n",
        "        \n",
        "        #output = [batch size, emb dim + hid dim * 2]\n",
        "        \n",
        "        prediction = self.fc_out(output)\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQTJY426UXYQ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> # Ponemos todo junto\n",
        "\n",
        "<h2>Spanish</h2>\n",
        "\n",
        "El foward pass general de nuestro modelo consiste en realizar una predicción en nuestro encoder y luego iterarivamente realizar predicciones en el decoder, ¿Hasta cuando? hasta que el modelo prediga el token que denota el final de la oracion < e o s > en la primera iteración del decoder tendremos como input el token inicial < s o s > pero en la siguiente iteraciòn el input será el output que acabamos de predecir. \n",
        "\n",
        "Hay una pequeña policy llamada Teacher Forcing que dice si con una probalidad de n por ciento utilizaremos el actual proximo token como input de lo contrario usamos el token predicho.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIHT_0KGWMXz",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> # Put all together\n",
        "\n",
        "<h2>English</h2>\n",
        "\n",
        "The general foward pass of our model consists in making a prediction in our encoder and then iterarily making predictions in the decoder, Until when? until the model predicts the token that denotes the end of the sentence < e o s > in the first iteration of the decoder we will have as input the initial token < s o s > but in the following iteration the input will be the output that we have just predicted. \n",
        "\n",
        "There is a little policy called Teacher Forcing that says if with a probability of n percent we will use the current next token as input otherwise we use the predicted token.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYxryGdq46th",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #last hidden state of the encoder is the context\n",
        "        hidden = self.encoder(src)\n",
        "        \n",
        "        #context also used as the initial hidden state of the decoder\n",
        "             \n",
        "        context = hidden\n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden state and the context state\n",
        "            #receive output tensor (predictions) and new hidden state\n",
        "            output, hidden = self.decoder(input, hidden, context)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riqU3zTQ9FBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFc9xjLa9HB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train phase\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, test_data), \n",
        "    batch_size = BATCH_SIZE, \n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgaW_eZE9kdu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_trKHXV_jBh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "outputId": "5d7214d7-c82f-4336-fa9e-98e1b26dab86"
      },
      "source": [
        "def init_weights(m):\n",
        "  for name, param in m.named_parameters():\n",
        "    nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(1555, 256)\n",
              "    (rnn): GRU(256, 512)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(21, 256)\n",
              "    (rnn): GRU(768, 512)\n",
              "    (fc_out): Linear(in_features=1280, out_features=21, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ffg5ByS3xz2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def solve_problem(model, SRC, TRG, sentence, device, max_length=50):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  spacy_es = spacy.load('es')\n",
        "\n",
        "  if type(sentence) == str:\n",
        "    tokens = [tok.text.lower() for tok in spacy_es(sentence)]\n",
        "  else:\n",
        "    [token.lower() for token in sentence]\n",
        "\n",
        "  tokens.insert(0, SRC.init_token)\n",
        "  tokens.append(SRC.eos_token)\n",
        "\n",
        "  text_to_indices = [SRC.vocab.stoi[token] for token in tokens]\n",
        "\n",
        "  sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "  \n",
        "  with torch.no_grad():\n",
        "\n",
        "      hidden = model.encoder(sentence_tensor)\n",
        "      context = hidden \n",
        "\n",
        "  outputs = [TRG.vocab.stoi[\"<sos>\"]]\n",
        "\n",
        "  for _ in range(max_length):\n",
        "    previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      output, hidden = model.decoder(previous_word, hidden, context)\n",
        "      best_guess = output.argmax(1).item()\n",
        "\n",
        "    outputs.append(best_guess)\n",
        "\n",
        "        # Model predicts it's the end of the sentence\n",
        "    if output.argmax(1).item() == TRG.vocab.stoi[\"<eos>\"]:\n",
        "      break\n",
        "\n",
        "  translated_sentence = [TRG.vocab.itos[idx] for idx in outputs]\n",
        "\n",
        "    # remove start token\n",
        "  return translated_sentence[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5fGfkaxAOVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLWtr8fcA1lg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.SRC\n",
        "        trg = batch.TRG\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg)\n",
        " \n",
        "        \n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaWB5AgeCDZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXGy726_BsKm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "732a0e1f-dc00-4d49-dcef-c68893aeaf46"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "N_EPOCHS = 20\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    sentence = \"Bridget tiene 4 chicles. Henry tiene 4 chicles. Si Henry le da todos sus chicles a Bridget, ¿cuántos chicles tendrá Bridget?\"\n",
        "    solve = solve_problem(model, SRC, TRG, sentence, device)\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)  \n",
        "  \n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print('Sentece {}, Result {}'.format(sentence, solve))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 19s\n",
            "\tTrain Loss: 2.661 | Train PPL:  14.305\n",
            "Sentece Bridget tiene 4 chicles. Henry tiene 4 chicles. Si Henry le da todos sus chicles a Bridget, ¿cuántos chicles tendrá Bridget?, Result ['1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1']\n",
            "Epoch: 02 | Time: 0m 19s\n",
            "\tTrain Loss: 2.084 | Train PPL:   8.035\n",
            "Sentece Bridget tiene 4 chicles. Henry tiene 4 chicles. Si Henry le da todos sus chicles a Bridget, ¿cuántos chicles tendrá Bridget?, Result ['x', '=', '.', '0', '.', '0', '0', '.', '0', '0', '<eos>']\n",
            "Epoch: 03 | Time: 0m 19s\n",
            "\tTrain Loss: 1.845 | Train PPL:   6.327\n",
            "Sentece Bridget tiene 4 chicles. Henry tiene 4 chicles. Si Henry le da todos sus chicles a Bridget, ¿cuántos chicles tendrá Bridget?, Result ['x', '=', '.', '0', '0', '0', '<eos>']\n",
            "Epoch: 04 | Time: 0m 18s\n",
            "\tTrain Loss: 1.704 | Train PPL:   5.496\n",
            "Sentece Bridget tiene 4 chicles. Henry tiene 4 chicles. Si Henry le da todos sus chicles a Bridget, ¿cuántos chicles tendrá Bridget?, Result ['x', '=', '1', '.', '0', '.', '0', '<eos>']\n",
            "Epoch: 05 | Time: 0m 19s\n",
            "\tTrain Loss: 1.595 | Train PPL:   4.926\n",
            "Sentece Bridget tiene 4 chicles. Henry tiene 4 chicles. Si Henry le da todos sus chicles a Bridget, ¿cuántos chicles tendrá Bridget?, Result ['x', '=', '2', '.', '0', '.', '0', '<eos>']\n",
            "Epoch: 06 | Time: 0m 19s\n",
            "\tTrain Loss: 1.596 | Train PPL:   4.933\n",
            "Sentece Bridget tiene 4 chicles. Henry tiene 4 chicles. Si Henry le da todos sus chicles a Bridget, ¿cuántos chicles tendrá Bridget?, Result ['x', '=', '4', '.', '0', '+', '0', '<eos>']\n",
            "Epoch: 07 | Time: 0m 19s\n",
            "\tTrain Loss: 1.430 | Train PPL:   4.178\n",
            "Sentece Bridget tiene 4 chicles. Henry tiene 4 chicles. Si Henry le da todos sus chicles a Bridget, ¿cuántos chicles tendrá Bridget?, Result ['x', '=', '1', '.', '0', '-', '.', '0', '<eos>']\n",
            "Epoch: 08 | Time: 0m 19s\n",
            "\tTrain Loss: 1.435 | Train PPL:   4.200\n",
            "Sentece Bridget tiene 4 chicles. Henry tiene 4 chicles. Si Henry le da todos sus chicles a Bridget, ¿cuántos chicles tendrá Bridget?, Result ['x', '=', '1', '.', '0', '0', '-', '.', '0', '<eos>']\n",
            "Epoch: 09 | Time: 0m 19s\n",
            "\tTrain Loss: 1.315 | Train PPL:   3.725\n",
            "Sentece Bridget tiene 4 chicles. Henry tiene 4 chicles. Si Henry le da todos sus chicles a Bridget, ¿cuántos chicles tendrá Bridget?, Result ['x', '=', '1', '.', '.', '0', '/', '1', '.', '0', '<eos>']\n",
            "Epoch: 10 | Time: 0m 19s\n",
            "\tTrain Loss: 1.389 | Train PPL:   4.012\n",
            "Sentece Bridget tiene 4 chicles. Henry tiene 4 chicles. Si Henry le da todos sus chicles a Bridget, ¿cuántos chicles tendrá Bridget?, Result ['x', '=', '1', '.', '0', '+', '4', '.', '0', '<eos>']\n",
            "Epoch: 11 | Time: 0m 19s\n",
            "\tTrain Loss: 1.435 | Train PPL:   4.198\n",
            "Sentece Bridget tiene 4 chicles. Henry tiene 4 chicles. Si Henry le da todos sus chicles a Bridget, ¿cuántos chicles tendrá Bridget?, Result ['x', '=', '2', '.', '0', '+', '/', '1', '.', '0', '<eos>']\n",
            "Epoch: 12 | Time: 0m 19s\n",
            "\tTrain Loss: 1.445 | Train PPL:   4.241\n",
            "Sentece Bridget tiene 4 chicles. Henry tiene 4 chicles. Si Henry le da todos sus chicles a Bridget, ¿cuántos chicles tendrá Bridget?, Result ['x', '=', '1', '.', '.', '0', '+', '1', '.', '0', '<eos>']\n",
            "Epoch: 13 | Time: 0m 19s\n",
            "\tTrain Loss: 1.300 | Train PPL:   3.671\n",
            "Sentece Bridget tiene 4 chicles. Henry tiene 4 chicles. Si Henry le da todos sus chicles a Bridget, ¿cuántos chicles tendrá Bridget?, Result ['x', '=', '1', '.', '0', '+', '1', '.', '0', '<eos>']\n",
            "Epoch: 14 | Time: 0m 19s\n",
            "\tTrain Loss: 1.366 | Train PPL:   3.920\n",
            "Sentece Bridget tiene 4 chicles. Henry tiene 4 chicles. Si Henry le da todos sus chicles a Bridget, ¿cuántos chicles tendrá Bridget?, Result ['x', '=', '1', '2', '.', '0', '/', '1', '.', '0', '<eos>']\n",
            "Epoch: 15 | Time: 0m 19s\n",
            "\tTrain Loss: 1.306 | Train PPL:   3.692\n",
            "Sentece Bridget tiene 4 chicles. Henry tiene 4 chicles. Si Henry le da todos sus chicles a Bridget, ¿cuántos chicles tendrá Bridget?, Result ['x', '=', '1', '.', '0', '+', '3', '.', '.', '0', '<eos>']\n",
            "Epoch: 16 | Time: 0m 19s\n",
            "\tTrain Loss: 1.292 | Train PPL:   3.640\n",
            "Sentece Bridget tiene 4 chicles. Henry tiene 4 chicles. Si Henry le da todos sus chicles a Bridget, ¿cuántos chicles tendrá Bridget?, Result ['x', '=', '1', '2', '.', '0', '+', '1', '2', '.', '0', '<eos>']\n",
            "Epoch: 17 | Time: 0m 19s\n",
            "\tTrain Loss: 1.343 | Train PPL:   3.830\n",
            "Sentece Bridget tiene 4 chicles. Henry tiene 4 chicles. Si Henry le da todos sus chicles a Bridget, ¿cuántos chicles tendrá Bridget?, Result ['x', '=', '1', '.', '.', '0', '/', '1', '.', '0', '<eos>']\n",
            "Epoch: 18 | Time: 0m 19s\n",
            "\tTrain Loss: 1.283 | Train PPL:   3.608\n",
            "Sentece Bridget tiene 4 chicles. Henry tiene 4 chicles. Si Henry le da todos sus chicles a Bridget, ¿cuántos chicles tendrá Bridget?, Result ['x', '=', '1', '.', '0', '*', '3', '.', '0', '<eos>']\n",
            "Epoch: 19 | Time: 0m 19s\n",
            "\tTrain Loss: 1.193 | Train PPL:   3.297\n",
            "Sentece Bridget tiene 4 chicles. Henry tiene 4 chicles. Si Henry le da todos sus chicles a Bridget, ¿cuántos chicles tendrá Bridget?, Result ['x', '=', '1', '2', '.', '0', '-', '1', '.', '0', '<eos>']\n",
            "Epoch: 20 | Time: 0m 19s\n",
            "\tTrain Loss: 1.243 | Train PPL:   3.466\n",
            "Sentece Bridget tiene 4 chicles. Henry tiene 4 chicles. Si Henry le da todos sus chicles a Bridget, ¿cuántos chicles tendrá Bridget?, Result ['x', '=', '1', '.', '0', '+', '3', '.', '0', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uql5tZz4pxXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}